{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a4265a",
   "metadata": {},
   "source": [
    "# Demo: NOâ‚ƒ Emulator (Keras, synthetic data)\n",
    "This is a **demonstration** notebook to mimic your emulator pipeline. It:\n",
    "1. Generates synthetic data with your feature names\n",
    "2. Trains a small Keras MLP with **EarlyStopping** (similar flavour to AutoKeras)\n",
    "3. Reports metrics and compares to a LinearRegression baseline\n",
    "\n",
    "> Note: This is **self-contained** for review. Replace the model training with your\n",
    "exported AutoKeras model when available (e.g., `model = tf.keras.models.load_model(...)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0029c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Synthetic data size\n",
    "N = 5000  # keep this small enough for quick notebooks\n",
    "\n",
    "# Feature names match your training_order (auto.py)\n",
    "feature_names = [\n",
    "    'lon','lat','WCDepth','chl_interp','pft_interp','sstost_interp',\n",
    "    'temp_interp','d2m_interp','t2m_interp','sp_interp','tp_interp',\n",
    "    'u10_interp','v10_interp','ssrdc_interp','phyc_interp','pp_interp',\n",
    "    'closest_rorunoff','closest_rono3','closest_ronh4','closest_roo',\n",
    "    'closest_rop','closest_rosio2','bathy_interp','month','day'\n",
    "]\n",
    "\n",
    "# Scientifically plausible ranges (very rough, shelf-focused)\n",
    "lon = rng.uniform(-12, 10, N)           # degrees_east (AMM7-ish)\n",
    "lat = rng.uniform(48, 62, N)            # degrees_north\n",
    "WCDepth = rng.uniform(0, 5, N)          # m (surface sample depth)\n",
    "chl = rng.uniform(0.05, 10, N)          # mg m^-3\n",
    "pft = rng.uniform(0, 5, N)              # arbitrary unit (component of chl)\n",
    "sstost = rng.uniform(6, 20, N)          # degC (OSTIA SST)\n",
    "temp = rng.uniform(5, 18, N)            # degC (CMEMS thetao surface)\n",
    "d2m = rng.uniform(270, 295, N)          # K\n",
    "t2m = rng.uniform(270, 298, N)          # K\n",
    "sp = rng.uniform(9.9e4, 1.03e5, N)      # Pa\n",
    "tp = rng.exponential(2e-3, N)           # m/day precip (ERA5 daily)\n",
    "u10 = rng.normal(0, 6, N)               # m/s\n",
    "v10 = rng.normal(0, 6, N)               # m/s\n",
    "ssrdc = rng.uniform(20, 280, N)         # W m^-2 (clear-sky daily avg approx)\n",
    "phyc = rng.uniform(0.05, 10, N)         # mg m^-3 (total phytoplankton)\n",
    "pp = rng.uniform(50, 1500, N)           # mg C m^-2 d^-1 (ballpark)\n",
    "runoff = rng.exponential(0.1, N)        # arbitrary units\n",
    "rono3 = rng.exponential(0.05, N)        # mmol N m^-2 d^-1 equiv (toy)\n",
    "ronh4 = rng.exponential(0.03, N)\n",
    "roo = rng.exponential(0.02, N)\n",
    "rop = rng.exponential(0.01, N)\n",
    "rosio2 = rng.exponential(0.04, N)\n",
    "bathy = rng.uniform(10, 300, N)         # m (coastal to shelf)\n",
    "month = rng.integers(1, 13, N)\n",
    "day = rng.integers(1, 29, N)            # keep simple (28 days)\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'lon': lon, 'lat': lat, 'WCDepth': WCDepth, 'chl_interp': chl, 'pft_interp': pft,\n",
    "    'sstost_interp': sstost, 'temp_interp': temp, 'd2m_interp': d2m, 't2m_interp': t2m,\n",
    "    'sp_interp': sp, 'tp_interp': tp, 'u10_interp': u10, 'v10_interp': v10,\n",
    "    'ssrdc_interp': ssrdc, 'phyc_interp': phyc, 'pp_interp': pp,\n",
    "    'closest_rorunoff': runoff, 'closest_rono3': rono3, 'closest_ronh4': ronh4,\n",
    "    'closest_roo': roo, 'closest_rop': rop, 'closest_rosio2': rosio2,\n",
    "    'bathy_interp': bathy, 'month': month, 'day': day\n",
    "})[feature_names]\n",
    "\n",
    "# Create a synthetic \"true\" mapping to NO3_obs with physically-inspired correlations:\n",
    "# - Increase with runoff, rono3, ronh4, bathymetry shallower (inverse)\n",
    "# - Seasonal modulation via month, effect of temperature/pp/chl\n",
    "true_no3 = (\n",
    "    0.15*runoff + 0.8*rono3 + 0.5*ronh4\n",
    "    + 0.002*(chl + phyc) - 0.03*(sstost-12).clip(min=-5, max=10)\n",
    "    + 0.0005*pp\n",
    "    + 0.002*(u10**2 + v10**2)\n",
    "    + 0.01*(14 - (bathy/50))              # shallower => higher\n",
    "    + 0.2*np.sin(2*np.pi*(month/12))\n",
    ")\n",
    "\n",
    "# Add heteroscedastic noise (higher variance with higher pp/chl)\n",
    "noise = rng.normal(0, 0.2 + 0.0002*pp + 0.005*chl, N)\n",
    "no3_obs = np.clip(true_no3 + noise, 0, None)  # prevent negatives\n",
    "\n",
    "# A simple \"model no3\" proxy to emulate a biased baseline (CMEMS/ERSEM-like)\n",
    "no3_model = np.clip(true_no3 * 0.85 + rng.normal(0, 0.3, N), 0, None)\n",
    "\n",
    "# Provide the dataframe in same structure you'd expect during training\n",
    "df = X.copy()\n",
    "df['no3_obs'] = no3_obs\n",
    "df['no3_model'] = no3_model\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    return float(np.mean(y_pred - y_true))\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    # Pearson correlation\n",
    "    yt = np.asarray(y_true).ravel()\n",
    "    yp = np.asarray(y_pred).ravel()\n",
    "    if yt.std() == 0 or yp.std() == 0:\n",
    "        return float('nan')\n",
    "    return float(np.corrcoef(yt, yp)[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_parity(y_true, y_pred, title):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
    "    minv = min(np.min(y_true), np.min(y_pred))\n",
    "    maxv = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([minv, maxv], [minv, maxv], lw=1)\n",
    "    plt.xlabel(\"Observed NO$_3$ [mmol N m$^{-3}$]\")\n",
    "    plt.ylabel(\"Predicted NO$_3$ [mmol N m$^{-3}$]\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "features = df.drop(columns=['no3_obs','no3_model'])\n",
    "target = df['no3_obs']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=11)\n",
    "\n",
    "# -------- Baseline (for comparison) --------\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_lr = lr.predict(X_test)\n",
    "\n",
    "# -------- Tiny Keras Emulator --------\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=1e-4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=[es],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "y_ml = model.predict(X_test, verbose=0).ravel()\n",
    "\n",
    "print(\"Linear Regression baseline:\")\n",
    "print(f\"  RMSE = {rmse(y_test, y_lr):.3f} mmol N m^-3\")\n",
    "print(f\"  Bias = {bias(y_test, y_lr):.3f} mmol N m^-3\")\n",
    "print(f\"  Corr = {corr(y_test, y_lr):.3f}\")\n",
    "\n",
    "print(\"\\nTiny Keras emulator:\")\n",
    "print(f\"  RMSE = {rmse(y_test, y_ml):.3f} mmol N m^-3\")\n",
    "print(f\"  Bias = {bias(y_test, y_ml):.3f} mmol N m^-3\")\n",
    "print(f\"  Corr = {corr(y_test, y_ml):.3f}\")\n",
    "\n",
    "plot_parity(y_test, y_lr, title=\"Baseline parity: Linear Regression (test set)\")\n",
    "plot_parity(y_test, y_ml, title=\"Emulator parity: Tiny Keras MLP (test set)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
